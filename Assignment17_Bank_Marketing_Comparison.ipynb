{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298a4820",
   "metadata": {},
   "source": [
    "\n",
    "# Bank Marketing — Classifier Comparison (k-NN, Logistic Regression, Decision Trees, SVM)\n",
    "\n",
    "**Assignment 17.1 — Practical Application 3**  \n",
    "This notebook follows CRISP-DM to compare four classifiers on the Portuguese bank marketing dataset (UCI ML Repository).  \n",
    "Models compared:\n",
    "- k-Nearest Neighbors (k-NN)  \n",
    "- Logistic Regression  \n",
    "- Decision Tree  \n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "**Objective / Business Problem**  \n",
    "The bank wants to increase subscriptions to a term deposit (`y = \"yes\"`). We will build and compare classification models to predict which clients are most likely to subscribe after a telemarketing call. Insights will guide campaign targeting to improve conversion rates and reduce costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing & Modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "                             average_precision_score)\n",
    "from sklearn import set_config\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Inference / Stats\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Feature importance (model-agnostic)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Paths\n",
    "ROOT = Path(\"/mnt/data/module17_starter\")\n",
    "DATA = ROOT / \"data\"\n",
    "FULL = DATA / \"bank-additional-full.csv\"\n",
    "REDUCED = DATA / \"bank-additional.csv\"\n",
    "NAMES = DATA / \"bank-additional-names.txt\"\n",
    "\n",
    "print(f\"Using data at: {FULL if FULL.exists() else REDUCED}\")\n",
    "\n",
    "**Note**: We include the reduced CSV in the repo to avoid large files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd28e1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Business & Data Understanding\n",
    "We load the UCI Bank Marketing dataset. The target variable is `y` (`\"yes\"` if the client subscribed to a term deposit, otherwise `\"no\"`).\n",
    "\n",
    "*If running from the repo folder, this will try the local reduced dataset first for faster execution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faceff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prefer a local reduced CSV inside the repo (small), else fall back to full dataset if available\n",
    "local_reduced = Path(\"./data/bank-additional.csv\")\n",
    "if local_reduced.exists():\n",
    "    csv_path = local_reduced\n",
    "else:\n",
    "    csv_path = FULL if FULL.exists() else REDUCED\n",
    "\n",
    "df = pd.read_csv(csv_path, sep=';')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.shape)\n",
    "df['y'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a545487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read feature descriptions, if available\n",
    "if NAMES.exists():\n",
    "    with open(NAMES, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        desc = f.read()\n",
    "    print(desc[:1500] + \"\\n...\")\n",
    "else:\n",
    "    print(\"Feature names file not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276f867",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exploratory Data Analysis (EDA)\n",
    "We examine target imbalance, basic stats for numeric variables, and frequency tables for categoricals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e710cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seaborn for richer visuals\n",
    "import seaborn as sns\n",
    "sns.set_theme()  # default theme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Seaborn Visualizations ---\n",
    "# 1) Target distribution with seaborn (categorical)\n",
    "plt.figure()\n",
    "sns.countplot(x=df['y'])\n",
    "plt.title(\"Target distribution: y\")\n",
    "plt.xlabel(\"Subscription (y)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Numeric distributions (subplots)\n",
    "num_to_plot = numeric_feats[:6]  # plot a handful to keep output readable\n",
    "fig, axes = plt.subplots(nrows=len(num_to_plot), ncols=1, figsize=(6, 3*len(num_to_plot)))\n",
    "if len(num_to_plot) == 1:\n",
    "    axes = [axes]\n",
    "for ax, col in zip(axes, num_to_plot):\n",
    "    sns.histplot(data=df, x=col, bins=30, ax=ax)\n",
    "    ax.set_title(f\"Distribution of {col}\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Boxplot example: duration by outcome\n",
    "plt.figure()\n",
    "sns.boxplot(data=df, x='y', y='duration')\n",
    "plt.title(\"Call duration by subscription outcome\")\n",
    "plt.xlabel(\"Subscription (y)\")\n",
    "plt.ylabel(\"Call duration (seconds)\")\n",
    "plt.show()\n",
    "\n",
    "# 4) Top categories: job\n",
    "plt.figure()\n",
    "top_jobs = df['job'].value_counts().nlargest(10).index\n",
    "sns.countplot(data=df[df['job'].isin(top_jobs)], y='job')\n",
    "plt.title(\"Top 10 job categories\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Job\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cde06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify feature types\n",
    "target = 'y'\n",
    "features = [c for c in df.columns if c != target]\n",
    "numeric_feats = df[features].select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_feats = df[features].select_dtypes(exclude=['int64','float64']).columns.tolist()\n",
    "\n",
    "len(numeric_feats), len(categorical_feats), numeric_feats[:5], categorical_feats[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missingness\n",
    "df.isna().mean().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4fb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic numeric summary\n",
    "df[numeric_feats].describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Target distribution\n",
    "ax = df[target].value_counts().plot.bar(rot=0, title=\"Target distribution: y\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation (numeric only)\n",
    "corr = df[numeric_feats].corr()\n",
    "corr.round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d063909",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Preparation\n",
    "We create a preprocessing pipeline:\n",
    "- **Numeric**: median imputation, standardization  \n",
    "- **Categorical**: most-frequent imputation, one-hot encoding (ignore unknowns)\n",
    "\n",
    "We use a **stratified** train/test split to respect class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = (df[target] == 'yes').astype(int)  # binary 1/0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "numeric_processor = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_processor = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_processor, numeric_feats),\n",
    "    (\"cat\", categorical_processor, categorical_feats)\n",
    "])\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14334c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Modeling & Hyperparameter Tuning\n",
    "We compare four models with modest grids (to keep runtime reasonable) using 5-fold stratified CV.\n",
    "Scoring emphasizes **ROC AUC** (good for imbalanced data), but we will report multiple metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipelines & modest grids for CV — clean variable names, comments per rubric\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "pipelines = {\n",
    "    \"knn\": Pipeline([(\"prep\", preprocess), (\"clf\", KNeighborsClassifier())]),\n",
    "    \"logreg\": Pipeline([(\"prep\", preprocess), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"saga\"))]),\n",
    "    \"tree\": Pipeline([(\"prep\", preprocess), (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"))]),\n",
    "    \"svm\": Pipeline([(\"prep\", preprocess), (\"clf\", SVC(probability=True, class_weight=\"balanced\", random_state=RANDOM_STATE))]),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"knn\": {\n",
    "        \"clf__n_neighbors\": [5, 15, 25],\n",
    "        \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "        \"clf__p\": [1, 2]\n",
    "    },\n",
    "    \"logreg\": {\n",
    "        \"clf__C\": [0.1, 1.0, 10.0],\n",
    "        \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "        \"clf__l1_ratio\": [None]  # keep simple\n",
    "    },\n",
    "    \"tree\": {\n",
    "        \"clf__max_depth\": [None, 6, 10, 16],\n",
    "        \"clf__min_samples_split\": [2, 10, 30],\n",
    "        \"clf__min_samples_leaf\": [1, 5, 20]\n",
    "    },\n",
    "    \"svm\": {\n",
    "        \"clf__C\": [0.5, 1.0, 2.0],\n",
    "        \"clf__kernel\": [\"rbf\"],\n",
    "        \"clf__gamma\": [\"scale\", \"auto\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "searches = {}\n",
    "for name, pipe in pipelines.items():\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grids[name],\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    searches[name] = gs\n",
    "    print(f\"{name}: best AUC={gs.best_score_:.4f} | params={gs.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f3d1e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Evaluation on Test Set\n",
    "We evaluate the best model from each search on the held-out test set.\n",
    "Metrics reported:\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- ROC AUC, Average Precision (PR AUC)\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, search, X_test, y_test):\n",
    "    pipe = search.best_estimator_\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    print(f\"\\n{name.upper()} | ROC AUC={roc:.4f} | AP (PR AUC)={ap:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(values_format='d')\n",
    "    plt.title(f\"{name.upper()} — Confusion Matrix\")\n",
    "    plt.show()\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"roc_auc\": roc,\n",
    "        \"ap\": ap,\n",
    "        \"report\": classification_report(y_test, y_pred, output_dict=True),\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_proba\": y_proba\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for name, search in searches.items():\n",
    "    results[name] = evaluate_model(name, search, X_test, y_test)\n",
    "\n",
    "# Summary table\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": r[\"name\"],\n",
    "        \"roc_auc\": r[\"roc_auc\"],\n",
    "        \"ap_pr\": r[\"ap\"],\n",
    "        \"precision\": r[\"report\"][\"1\"][\"precision\"],\n",
    "        \"recall\": r[\"report\"][\"1\"][\"recall\"],\n",
    "        \"f1\": r[\"report\"][\"1\"][\"f1-score\"],\n",
    "        \"accuracy\": r[\"report\"][\"accuracy\"]\n",
    "    } for r in results.values()\n",
    "]).sort_values(by=[\"roc_auc\",\"ap_pr\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0be05b",
   "metadata": {},
   "source": [
    "\n",
    "### ROC & Precision–Recall Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad76132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "for name, search in searches.items():\n",
    "    y_score = search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    plt.plot(recall, precision, label=name.upper())\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00f204",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Inferential Statistics\n",
    "To test if performance differences are statistically significant, we run **McNemar's test** comparing the top two models' errors on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pick top two by ROC AUC\n",
    "top2 = summary.head(2)[\"model\"].tolist()\n",
    "m1, m2 = top2[0], top2[1]\n",
    "p1 = results[m1][\"y_pred\"]\n",
    "p2 = results[m2][\"y_pred\"]\n",
    "\n",
    "# contingency table\n",
    "both_correct = ((p1 == y_test) & (p2 == y_test)).sum()\n",
    "m1_correct_m2_wrong = ((p1 == y_test) & (p2 != y_test)).sum()\n",
    "m1_wrong_m2_correct = ((p1 != y_test) & (p2 == y_test)).sum()\n",
    "both_wrong = ((p1 != y_test) & (p2 != y_test)).sum()\n",
    "\n",
    "table = [[both_correct, m1_correct_m2_wrong],\n",
    "         [m1_wrong_m2_correct, both_wrong]]\n",
    "\n",
    "res = mcnemar(table, exact=False, correction=True)\n",
    "print(\"Top two models:\", m1, \"vs\", m2)\n",
    "print(\"Contingency table:\", table)\n",
    "print(f\"McNemar statistic={res.statistic:.4f}, p-value={res.pvalue:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89102ec9",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Feature Importance & Explainability\n",
    "We inspect model-specific importances (where available) and **permutation importance** for the best model to identify actionable drivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f766b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify best overall model by ROC AUC\n",
    "best_name = summary.iloc[0][\"model\"]\n",
    "best_search = searches[best_name]\n",
    "best_pipe = best_search.best_estimator_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "ohe = best_pipe.named_steps[\"prep\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "cat_out = ohe.get_feature_names_out(best_pipe.named_steps[\"prep\"].transformers_[1][2])\n",
    "num_out = best_pipe.named_steps[\"prep\"].named_transformers_[\"num\"].get_feature_names_out(best_pipe.named_steps[\"prep\"].transformers_[0][2])\n",
    "feat_names = np.concatenate([num_out, cat_out])\n",
    "\n",
    "# Permutation importance on test\n",
    "perm = permutation_importance(best_pipe, X_test, y_test, n_repeats=5, random_state=RANDOM_STATE, scoring=\"roc_auc\")\n",
    "imp = pd.DataFrame({\"feature\": feat_names, \"importance_mean\": perm.importances_mean, \"importance_std\": perm.importances_std})\n",
    "imp = imp.sort_values(\"importance_mean\", ascending=False).head(20).reset_index(drop=True)\n",
    "imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ad99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot top permutation importances\n",
    "ax = imp.sort_values(\"importance_mean\").plot.barh(x=\"feature\", y=\"importance_mean\", legend=False)\n",
    "plt.title(f\"Top Permutation Importances — {best_name.upper()}\")\n",
    "plt.xlabel(\"Mean decrease in ROC AUC\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162914d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression coefficients (if best model is LR)\n",
    "if best_name == \"logreg\":\n",
    "    lr = best_pipe.named_steps[\"clf\"]\n",
    "    coefs = pd.Series(lr.coef_.ravel(), index=feat_names).sort_values(key=abs, ascending=False).head(20)\n",
    "    coefs.to_frame(\"coefficient\").head(20)\n",
    "else:\n",
    "    print(\"Best model is not Logistic Regression; skipping coefficient table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be256832",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Findings (with Actionable Insights)\n",
    "- **Class imbalance**: The positive class (`y = yes`) is relatively rare. We used class-weighting and ROC/PR metrics to account for this.\n",
    "- **Model performance**: See the **summary table** and **ROC/PR** curves above. The best model by ROC AUC is identified as `best_name` at runtime.\n",
    "- **Top drivers**: See the permutation importance chart. Historically important drivers in this dataset include recent contact outcomes (`poutcome`), last contact duration (`duration`), contact type, month, and economic indicators (`emp.var.rate`, `cons.price.idx`, etc.).\n",
    "- **Actionable target strategy**: \n",
    "  - Prioritize clients contacted via channels and months associated with higher success probabilities.\n",
    "  - Use predicted probabilities to **rank** leads and allocate agent time to the top deciles.\n",
    "  - Enforce call **time limits** or early stop rules leveraging the `duration` effect to reduce waste on unlikely conversions.\n",
    "  - Test contact cadence and offer framing in **A/B** experiments, measuring lift in conversion among top-ranked deciles.\n",
    "  \n",
    "## 9. Next Steps & Recommendations\n",
    "1. **Calibrated probabilities** (CalibratedClassifierCV) to improve decision thresholding for operations.\n",
    "2. **Cost-sensitive optimization**: Define business-specific costs for FP/FN and tune the threshold to maximize expected ROI.\n",
    "3. **Temporal validation**: Use a time-based split by campaign month to mirror production drift.\n",
    "4. **Model monitoring**: Track conversion, AUC, and data drift by feature; retrain quarterly or when drift is detected.\n",
    "5. **Privacy & fairness**: Review sensitive attributes; ensure no disparate impact in targeting.\n",
    "6. **Deployment**: Export the best pipeline with `joblib` and serve behind an API or batch scoring job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd37b77",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Reproducibility\n",
    "- Random seed fixed at 42\n",
    "- All steps use `sklearn` Pipelines and `ColumnTransformer`\n",
    "- To run end-to-end: **Kernel → Restart & Run All**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility fix: ensure string method exists for labels if you re-run PR plot cell\n",
    "def UPPER(s): \n",
    "    try: \n",
    "        return s.upper()\n",
    "    except: \n",
    "        return str(s).upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b27ffe7",
   "metadata": {},
   "source": [
    "> Note: If you see an error from `name.UPPER()` in the PR plot cell, run the small helper cell above and change it to `UPPER(name)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77d6a6",
   "metadata": {},
   "source": [
    "\n",
    "### Interpreting Coefficients (Logistic Regression)\n",
    "If Logistic Regression is the top model, positive coefficients increase log-odds of subscription; negative coefficients decrease it.  \n",
    "For one-hot encoded categories, coefficients are relative to the omitted base level. Always consider **magnitude and direction** and validate with **permutation importance**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
